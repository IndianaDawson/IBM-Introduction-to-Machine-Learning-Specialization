{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Machine Learning Foundation\n\n## Section 2, Part c: Cross Validation "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Learning objectives\n\nBy the end of this lesson, you will be able to:\n\n* Chain multiple data processing steps together using `Pipeline`\n* Use the `KFolds` object to split data into multiple folds.\n* Perform cross validation using SciKit Learn with `cross_val_predict` and `GridSearchCV`\n"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:20:55.865735Z",
                    "start_time": "2019-02-19T17:20:54.685698Z"
                }
            },
            "outputs": [],
            "source": "import numpy as np\nimport pickle\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport types\nfrom botocore.client import Config\nimport ibm_boto3\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold, cross_val_predict\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import Pipeline"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "# make sure the credentials below in the def match the credentials you loaded above\ndef download_file_cos(credentials,local_file_name,key):  \n    cos = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials['ENDPOINT'])\n    try:\n        res=cos.download_file(Bucket=credentials['BUCKET'],Key=key,Filename=local_file_name)\n    except Exception as e:\n        print(Exception, e)\n    else:\n        print('The file %s has been downloaded into this notebook' % key)"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "The file boston_housing_clean.pickle has been downloaded into this notebook\n"
                }
            ],
            "source": "# For local file, input the name of what you want the file to be, for the key put name of the file on Cloud Object Storage\n# it would be best if they match for ease\ndownload_file_cos(credentials,'boston_housing_clean.pickle','boston_housing_clean.pickle')"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:20:58.445911Z",
                    "start_time": "2019-02-19T17:20:58.440191Z"
                }
            },
            "outputs": [],
            "source": "# Note we are loading a slightly different (\"cleaned\") pickle file\nboston = pickle.load(open('boston_housing_clean.pickle', \"rb\" ))"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:21:01.676948Z",
                    "start_time": "2019-02-19T17:21:01.671315Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "dict_keys(['dataframe', 'description'])"
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "boston.keys()"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:21:10.682135Z",
                    "start_time": "2019-02-19T17:21:10.678835Z"
                }
            },
            "outputs": [],
            "source": "boston_data = boston['dataframe']\nboston_description = boston['description']"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:22:10.274974Z",
                    "start_time": "2019-02-19T17:22:10.256879Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>MEDV</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n\n   PTRATIO       B  LSTAT  MEDV  \n0     15.3  396.90   4.98  24.0  \n1     17.8  396.90   9.14  21.6  \n2     17.8  392.83   4.03  34.7  \n3     18.7  394.63   2.94  33.4  \n4     18.7  396.90   5.33  36.2  "
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "boston_data.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Discussion: \n\nSuppose we want to do Linear Regression on our dataset to get an estimate, based on mean squared error, of how well our model will perform on data outside our dataset. \n\nSuppose also that our data is split into three folds: Fold 1, Fold 2, and Fold 3.\n\nWhat would the steps be, in English, to do this?"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Your response below**"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": " "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Coding this up\n\nThe [`KFold`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) object in SciKit Learn tells the cross validation object (see below) how to split up the data:"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:23:05.068445Z",
                    "start_time": "2019-02-19T17:23:05.064683Z"
                }
            },
            "outputs": [],
            "source": "X = boston_data.drop('MEDV', axis=1)\ny = boston_data.MEDV"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:23:10.538982Z",
                    "start_time": "2019-02-19T17:23:10.534325Z"
                }
            },
            "outputs": [],
            "source": "kf = KFold(shuffle=True, random_state=72018, n_splits=3)"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:25:04.578536Z",
                    "start_time": "2019-02-19T17:25:04.568959Z"
                },
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train index: [ 1  3  4  5  7  8 10 11 12 13] 337\nTest index: [ 0  2  6  9 15 17 19 23 25 26] 169\n\nTrain index: [ 0  2  6  9 10 11 12 13 15 17] 337\nTest index: [ 1  3  4  5  7  8 14 16 22 27] 169\n\nTrain index: [0 1 2 3 4 5 6 7 8 9] 338\nTest index: [10 11 12 13 18 20 21 24 28 31] 168\n\n"
                }
            ],
            "source": "for train_index, test_index in kf.split(X):\n    print(\"Train index:\", train_index[:10], len(train_index))\n    print(\"Test index:\",test_index[:10], len(test_index))\n    print('')"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:28:10.441616Z",
                    "start_time": "2019-02-19T17:28:10.204857Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "[0.671934879847278, 0.7485020059212362, 0.6976807323597782]"
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "#from sklearn.metrics import r2_score, mean_squared_error\n\nscores = []\nlr = LinearRegression()\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test, y_train, y_test = (X.iloc[train_index, :], \n                                        X.iloc[test_index, :], \n                                        y[train_index], \n                                        y[test_index])\n    \n    lr.fit(X_train, y_train)\n        \n    y_pred = lr.predict(X_test)\n\n    score = r2_score(y_test.values, y_pred)\n    \n    scores.append(score)\n    \nscores"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "A bit cumbersome, but do-able."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Discussion (Part 2): \n\nNow suppose we want to do the same, but appropriately scaling our data as we go through the folds.\n\nWhat would the steps be _now_?"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Your response below**"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Coding this up"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:32:07.846336Z",
                    "start_time": "2019-02-19T17:32:07.808810Z"
                }
            },
            "outputs": [],
            "source": "scores = []\n\nlr = LinearRegression()\ns = StandardScaler()\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test, y_train, y_test = (X.iloc[train_index, :], \n                                        X.iloc[test_index, :], \n                                        y[train_index], \n                                        y[test_index])\n    \n    X_train_s = s.fit_transform(X_train)\n    \n    lr.fit(X_train_s, y_train)\n    \n    X_test_s = s.transform(X_test)\n    \n    y_pred = lr.predict(X_test_s)\n\n    score = r2_score(y_test.values, y_pred)\n    \n    scores.append(score)"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:32:09.978972Z",
                    "start_time": "2019-02-19T17:32:09.974341Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "[0.6719348798472715, 0.748502005921238, 0.6976807323597742]"
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "scores"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "(same scores, because for vanilla linear regression with no regularization, scaling actually doesn't matter for performance)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "This is getting quite cumbersome! \n\n_Very_ luckily, SciKit Learn has some wonderful functions that handle a lot of this for us."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### `Pipeline` and `cross_val_predict`"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "`Pipeline` lets you chain together multiple operators on your data that both have a `fit` method."
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:33:28.695381Z",
                    "start_time": "2019-02-19T17:33:28.691865Z"
                }
            },
            "outputs": [],
            "source": "s = StandardScaler()\nlr = LinearRegression()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Combine multiple processing steps into a `Pipeline`\n\nA pipeline contains a series of steps, where a step is (\"name of step\", actual_model). The \"name of step\" string is only used to help you identify which step you are on, and to allow you to specify parameters at that step.  "
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:33:32.657281Z",
                    "start_time": "2019-02-19T17:33:32.653852Z"
                }
            },
            "outputs": [],
            "source": "estimator = Pipeline([(\"scaler\", s),\n                      (\"regression\", lr)])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### `cross_val_predict`\n\n[`cross_val_predict`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) is a function that does K-fold cross validation for us, appropriately fitting and transforming at every step of the way."
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "KFold(n_splits=3, random_state=72018, shuffle=True)"
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "kf"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:35:55.255356Z",
                    "start_time": "2019-02-19T17:35:55.240376Z"
                }
            },
            "outputs": [],
            "source": "predictions = cross_val_predict(estimator, X, y, cv=kf)"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:36:03.801732Z",
                    "start_time": "2019-02-19T17:36:03.796646Z"
                },
                "scrolled": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.7063531064161559"
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "r2_score(y, predictions)"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:36:19.831481Z",
                    "start_time": "2019-02-19T17:36:19.827131Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.7060392060427612"
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "np.mean(scores) # almost identical!"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Note that `cross_val_predict` doesn't use the same model for all steps; the predictions for each row are made when that row is in the validation set. We really have the collected results of 3 (i.e. `kf.num_splits`) different models. \n\nWhen we are done, `estimator` is still not fitted. If we want to predict on _new_ data, we still have to train our `estimator`. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Hyperparameter tuning"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Definition\n\n**Hyperparameter tuning** involves using cross validation (or train-test split) to determine which hyperparameters are most likely to generate a model that _generalizes_ well outside of your sample.\n\n### Mechanics\n\nWe can generate an exponentially spaces range of values using the numpy [`geomspace`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.geomspace.html#numpy.geomspace) function.\n\n```python\nnp.geomspace(1, 1000, num=4)\n```\n\nproduces:\n\n```\narray([    1.,    10.,   100.,  1000.])\n```\n\nUse this function to generate a list of length 10 called `alphas` for hyperparameter tuning:"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:40:36.220744Z",
                    "start_time": "2019-02-19T17:40:36.214714Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([1.e-09, 1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02,\n       1.e-01, 1.e+00])"
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "alphas = np.geomspace(1e-9, 1e0, num=10)\nalphas"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The code below tunes the `alpha` hyperparameter for Lasso regression."
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:47:34.947390Z",
                    "start_time": "2019-02-19T17:47:34.845011Z"
                }
            },
            "outputs": [],
            "source": "scores = []\ncoefs = []\nfor alpha in alphas:\n    las = Lasso(alpha=alpha, max_iter=100000)\n    \n    estimator = Pipeline([\n        (\"scaler\", s),\n        (\"lasso_regression\", las)])\n\n    predictions = cross_val_predict(estimator, X, y, cv = kf)\n    \n    score = r2_score(y, predictions)\n    \n    scores.append(score)"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:47:35.397285Z",
                    "start_time": "2019-02-19T17:47:35.390917Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "[(1e-09, 0.7063531064981925),\n (1e-08, 0.7063531072356071),\n (1e-07, 0.7063531145602442),\n (1e-06, 0.7063531882052065),\n (1e-05, 0.7063539165191507),\n (0.0001, 0.706361268093463),\n (0.001, 0.706433467041546),\n (0.01, 0.7070865958083233),\n (0.1, 0.705838151167185),\n (1.0, 0.6512724532884887)]"
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "list(zip(alphas,scores))"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:48:16.943881Z",
                    "start_time": "2019-02-19T17:48:16.937741Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([-1.07170372e-01,  4.63952623e-02,  2.08588308e-02,  2.68854318e+00,\n       -1.77954207e+01,  3.80475296e+00,  7.50802707e-04, -1.47575348e+00,\n        3.05654279e-01, -1.23293755e-02, -9.53459908e-01,  9.39253013e-03,\n       -5.25467196e-01])"
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "Lasso(alpha=1e-6).fit(X, y).coef_"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:48:40.778026Z",
                    "start_time": "2019-02-19T17:48:40.771732Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([-0.06342255,  0.04916867, -0.        ,  0.        , -0.        ,\n        0.94678567,  0.02092737, -0.66900864,  0.26417501, -0.01520915,\n       -0.72319901,  0.00829117, -0.76143296])"
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "Lasso(alpha=1.0).fit(X, y).coef_"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:46:03.208768Z",
                    "start_time": "2019-02-19T17:46:02.898338Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 720x432 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "plt.figure(figsize=(10,6))\nplt.semilogx(alphas, scores, '-o')\nplt.xlabel('$\\\\alpha$')\nplt.ylabel('$R^2$');"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Exercise\n\nAdd `PolynomialFeatures` to this `Pipeline`, and re-run the cross validation with the `PolynomialFeatures` added.\n\n**Hint #1:** pipelines process input from first to last. Think about the order that it would make sense to add Polynomial Features to the data in sequence and add them in the appropriate place in the pipeline.\n\n**Hint #2:** you should see a significant increase in cross validation accuracy from doing this"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:56:02.883875Z",
                    "start_time": "2019-02-19T17:56:00.477593Z"
                }
            },
            "outputs": [],
            "source": "pf = PolynomialFeatures(degree=3)\n\nscores = []\nalphas = np.geomspace(0.06, 6.0, 20)\nfor alpha in alphas:\n    las = Lasso(alpha=alpha, max_iter=100000)\n    \n    estimator = Pipeline([\n        (\"scaler\", s),\n        (\"make_higher_degree\", pf),\n        (\"lasso_regression\", las)])\n\n    predictions = cross_val_predict(estimator, X, y, cv = kf)\n    \n    score = r2_score(y, predictions)\n    \n    scores.append(score)\n    "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "If you store the results in a list called `scores`, the following will work:"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:56:05.154628Z",
                    "start_time": "2019-02-19T17:56:04.987932Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 432x288 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "plt.semilogx(alphas, scores);"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:56:40.051147Z",
                    "start_time": "2019-02-19T17:56:40.009641Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.9134777735196521"
                    },
                    "execution_count": 29,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# Once we have found the hyperparameter (alpha~1e-2=0.01)\n# make the model and train it on ALL the data\n# Then release it into the wild .....\nbest_estimator = Pipeline([\n                    (\"scaler\", s),\n                    (\"make_higher_degree\", PolynomialFeatures(degree=2)),\n                    (\"lasso_regression\", Lasso(alpha=0.03))])\n\nbest_estimator.fit(X, y)\nbest_estimator.score(X, y)"
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:56:51.526585Z",
                    "start_time": "2019-02-19T17:56:51.521094Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([ 0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n        0.00000000e+00, -1.00309168e+00,  3.32679107e+00, -1.01840878e+00,\n       -2.56161421e+00,  1.12778302e+00, -1.72266155e+00, -5.37088506e-01,\n        4.39555878e-01, -3.39542586e+00,  7.22387712e-02,  0.00000000e+00,\n        0.00000000e+00,  3.53653554e+00, -0.00000000e+00,  3.72285440e-01,\n        0.00000000e+00,  0.00000000e+00, -5.49528703e-01, -0.00000000e+00,\n       -0.00000000e+00, -4.05522485e-02,  2.25864611e-01,  1.78508858e-01,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  6.50874606e-02,\n       -0.00000000e+00, -2.07295802e-01, -0.00000000e+00,  3.71781995e-01,\n        0.00000000e+00, -0.00000000e+00, -5.89531100e-02,  3.47180625e-01,\n        0.00000000e+00,  9.23666274e-01,  3.48873365e-01,  7.29463442e-02,\n        0.00000000e+00,  0.00000000e+00,  7.68485586e-02, -7.21083596e-01,\n        0.00000000e+00, -5.98542558e-01,  4.18420677e-01, -7.98165728e-01,\n       -7.25062683e-01,  2.34818861e-01, -0.00000000e+00, -0.00000000e+00,\n        0.00000000e+00, -1.68164447e-02,  0.00000000e+00, -4.04477826e-01,\n       -4.22989874e-01, -4.06983988e-01, -3.75443720e-01,  4.17684564e-01,\n       -8.91841193e-01,  0.00000000e+00, -2.69309481e-01,  0.00000000e+00,\n        1.02286785e-01,  2.02570379e-01, -6.88345376e-01, -0.00000000e+00,\n       -1.08598703e+00, -3.98751731e-01, -9.37684760e-01, -1.17343147e-01,\n       -7.37427594e-01,  0.00000000e+00,  0.00000000e+00,  1.36340670e+00,\n       -0.00000000e+00, -2.94691228e-03, -8.98125013e-01, -8.68198373e-01,\n        8.03396788e-01, -1.91683803e-01, -1.14706070e-01,  0.00000000e+00,\n       -0.00000000e+00,  5.83161589e-01, -0.00000000e+00,  5.81365491e-02,\n        0.00000000e+00, -2.32896159e-01, -1.12440837e+00,  0.00000000e+00,\n        1.96286997e+00, -0.00000000e+00, -1.00915801e+00, -7.04656486e-02,\n       -1.06456357e-02, -4.78389591e-02, -3.97645601e-01, -3.84121840e-01,\n        9.97402419e-01])"
                    },
                    "execution_count": 30,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "best_estimator.named_steps[\"lasso_regression\"].coef_"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Exercise\n\nDo the same, but with `Ridge` regression \n\nWhich model, `Ridge` or `Lasso`, performs best with its optimal hyperparameters on the Boston dataset?"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:59:17.634414Z",
                    "start_time": "2019-02-19T17:59:17.238684Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "[<matplotlib.lines.Line2D at 0x7efbf5c9ca20>]"
                    },
                    "execution_count": 31,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 432x288 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "pf = PolynomialFeatures(degree=2)\nalphas = np.geomspace(4, 20, 20)\nscores=[]\nfor alpha in alphas:\n    ridge = Ridge(alpha=alpha, max_iter=100000)\n\n    estimator = Pipeline([\n        (\"scaler\", s),\n        (\"polynomial_features\", pf),\n        (\"ridge_regression\", ridge)])\n\n    predictions = cross_val_predict(estimator, X, y, cv = kf)\n    score = r2_score(y, predictions)\n    scores.append(score)\n\nplt.plot(alphas, scores)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Conclusion:** Both Lasso and Ridge with proper hyperparameter tuning give better results than plain ol' Linear Regression!"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Exercise:"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now, for whatever your best overall hyperparameter was: \n\n* Standardize the data\n* Fit and predict on the entire dataset\n* See what the largest coefficients were\n    * Hint: use \n    ```python\n    dict(zip(model.coef_, pf.get_feature_names()))\n    ```\n    for your model `model` to get the feature names from `PolynomialFeatures`.\n    \n    Then, use\n    ```python\n    dict(zip(list(range(len(X.columns.values))), X.columns.values))\n    ```\n    \n    to see which features in the `PolynomialFeatures` DataFrame correspond to which columns in the original DataFrame.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:56:40.051147Z",
                    "start_time": "2019-02-19T17:56:40.009641Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n"
                },
                {
                    "data": {
                        "text/plain": "0.9241797703677183"
                    },
                    "execution_count": 32,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# Once we have found the hyperparameter (alpha~1e-2=0.01)\n# make the model and train it on ALL the data\n# Then release it into the wild .....\nbest_estimator = Pipeline([\n                    (\"scaler\", s),\n                    (\"make_higher_degree\", PolynomialFeatures(degree=2)),\n                    (\"lasso_regression\", Lasso(alpha=0.01))])\n\nbest_estimator.fit(X, y)\nbest_estimator.score(X, y)"
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T17:56:51.526585Z",
                    "start_time": "2019-02-19T17:56:51.521094Z"
                }
            },
            "outputs": [],
            "source": "df_importances = pd.DataFrame(zip(best_estimator.named_steps[\"make_higher_degree\"].get_feature_names(input_features=X.columns),\n                 best_estimator.named_steps[\"lasso_regression\"].coef_,\n))"
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": "col_names_dict = dict(zip(list(range(len(X.columns.values))), X.columns.values))"
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "{0: 'CRIM',\n 1: 'ZN',\n 2: 'INDUS',\n 3: 'CHAS',\n 4: 'NOX',\n 5: 'RM',\n 6: 'AGE',\n 7: 'DIS',\n 8: 'RAD',\n 9: 'TAX',\n 10: 'PTRATIO',\n 11: 'B',\n 12: 'LSTAT'}"
                    },
                    "execution_count": 35,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "col_names_dict"
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>90</th>\n      <td>RAD^2</td>\n      <td>-4.803638</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>LSTAT</td>\n      <td>-3.307996</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>DIS</td>\n      <td>-2.812789</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>AGE</td>\n      <td>-1.454097</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>DIS RAD</td>\n      <td>-1.431074</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>RAD LSTAT</td>\n      <td>-1.426539</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>RM TAX</td>\n      <td>-1.399134</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>AGE B</td>\n      <td>-1.355802</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>AGE LSTAT</td>\n      <td>-1.250545</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>NOX</td>\n      <td>-1.237913</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>TAX LSTAT</td>\n      <td>-1.120915</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>RM AGE</td>\n      <td>-0.986135</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>NOX AGE</td>\n      <td>-0.954266</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>RM LSTAT</td>\n      <td>-0.912320</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>RM PTRATIO</td>\n      <td>-0.907713</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>CHAS NOX</td>\n      <td>-0.905209</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>CHAS RM</td>\n      <td>-0.864616</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>NOX PTRATIO</td>\n      <td>-0.830786</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>CRIM RAD</td>\n      <td>-0.783356</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>NOX RAD</td>\n      <td>-0.771121</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>RM RAD</td>\n      <td>-0.714006</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>AGE TAX</td>\n      <td>-0.704531</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>DIS TAX</td>\n      <td>-0.684814</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>NOX^2</td>\n      <td>-0.623731</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>INDUS LSTAT</td>\n      <td>-0.603009</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>CRIM NOX</td>\n      <td>-0.514722</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>ZN LSTAT</td>\n      <td>-0.504731</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>PTRATIO</td>\n      <td>-0.480598</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>CHAS LSTAT</td>\n      <td>-0.437701</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>INDUS PTRATIO</td>\n      <td>-0.399364</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>AGE^2</td>\n      <td>0.102504</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>CHAS DIS</td>\n      <td>0.107129</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>CRIM^2</td>\n      <td>0.117544</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>INDUS TAX</td>\n      <td>0.150919</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>ZN^2</td>\n      <td>0.156627</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>RM^2</td>\n      <td>0.163943</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>INDUS B</td>\n      <td>0.165525</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>CHAS B</td>\n      <td>0.168054</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>CHAS AGE</td>\n      <td>0.228849</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>INDUS AGE</td>\n      <td>0.234004</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>CHAS^2</td>\n      <td>0.460487</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>INDUS^2</td>\n      <td>0.585708</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>NOX LSTAT</td>\n      <td>0.592488</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>CRIM RM</td>\n      <td>0.692385</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>DIS LSTAT</td>\n      <td>0.810410</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>LSTAT^2</td>\n      <td>0.822107</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>B</td>\n      <td>0.852722</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>CRIM LSTAT</td>\n      <td>0.885160</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>INDUS</td>\n      <td>0.895767</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>INDUS RM</td>\n      <td>0.937711</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>INDUS DIS</td>\n      <td>1.022539</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>ZN TAX</td>\n      <td>1.089461</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>NOX DIS</td>\n      <td>1.172946</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>DIS^2</td>\n      <td>1.281667</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>TAX PTRATIO</td>\n      <td>1.877365</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>AGE RAD</td>\n      <td>2.002135</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>INDUS NOX</td>\n      <td>2.360496</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>RM</td>\n      <td>3.429961</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>RAD TAX</td>\n      <td>4.083243</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>CRIM CHAS</td>\n      <td>4.599620</td>\n    </tr>\n  </tbody>\n</table>\n<p>105 rows \u00d7 2 columns</p>\n</div>",
                        "text/plain": "                 0         1\n90           RAD^2 -4.803638\n13           LSTAT -3.307996\n8              DIS -2.812789\n7              AGE -1.454097\n85         DIS RAD -1.431074\n94       RAD LSTAT -1.426539\n73          RM TAX -1.399134\n82           AGE B -1.355802\n83       AGE LSTAT -1.250545\n5              NOX -1.237913\n98       TAX LSTAT -1.120915\n70          RM AGE -0.986135\n62         NOX AGE -0.954266\n76        RM LSTAT -0.912320\n74      RM PTRATIO -0.907713\n51        CHAS NOX -0.905209\n52         CHAS RM -0.864616\n66     NOX PTRATIO -0.830786\n22        CRIM RAD -0.783356\n64         NOX RAD -0.771121\n72          RM RAD -0.714006\n80         AGE TAX -0.704531\n86         DIS TAX -0.684814\n60           NOX^2 -0.623731\n49     INDUS LSTAT -0.603009\n18        CRIM NOX -0.514722\n38        ZN LSTAT -0.504731\n11         PTRATIO -0.480598\n59      CHAS LSTAT -0.437701\n47   INDUS PTRATIO -0.399364\n..             ...       ...\n77           AGE^2  0.102504\n54        CHAS DIS  0.107129\n14          CRIM^2  0.117544\n46       INDUS TAX  0.150919\n27            ZN^2  0.156627\n69            RM^2  0.163943\n48         INDUS B  0.165525\n58          CHAS B  0.168054\n53        CHAS AGE  0.228849\n43       INDUS AGE  0.234004\n50          CHAS^2  0.460487\n39         INDUS^2  0.585708\n68       NOX LSTAT  0.592488\n19         CRIM RM  0.692385\n89       DIS LSTAT  0.810410\n104        LSTAT^2  0.822107\n12               B  0.852722\n26      CRIM LSTAT  0.885160\n3            INDUS  0.895767\n42        INDUS RM  0.937711\n44       INDUS DIS  1.022539\n35          ZN TAX  1.089461\n63         NOX DIS  1.172946\n84           DIS^2  1.281667\n96     TAX PTRATIO  1.877365\n79         AGE RAD  2.002135\n41       INDUS NOX  2.360496\n6               RM  3.429961\n91         RAD TAX  4.083243\n17       CRIM CHAS  4.599620\n\n[105 rows x 2 columns]"
                    },
                    "execution_count": 36,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df_importances.sort_values(by=1)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Grid Search CV"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "To do cross-validation, we used two techniques:\n- use `KFolds` and manually create a loop to do cross-validation\n- use `cross_val_predict` and `score` to get a cross-valiated score in a couple of lines.\n\nTo do hyper-parameter tuning, we see a general pattern:\n- use `cross_val_predict` and `score` in a manually written loop over hyperparemeters, then select the best one.\n\nPerhaps not surprisingly, there is a function that does this for us -- `GridSearchCV`"
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T18:02:31.945804Z",
                    "start_time": "2019-02-19T18:02:31.938416Z"
                }
            },
            "outputs": [],
            "source": "from sklearn.model_selection import GridSearchCV\n\n# Same estimator as before\nestimator = Pipeline([(\"scaler\", StandardScaler()),\n        (\"polynomial_features\", PolynomialFeatures()),\n        (\"ridge_regression\", Ridge())])\n\nparams = {\n    'polynomial_features__degree': [1, 2, 3],\n    'ridge_regression__alpha': np.geomspace(4, 20, 30)\n}\n\ngrid = GridSearchCV(estimator, params, cv=kf)"
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T18:02:49.319148Z",
                    "start_time": "2019-02-19T18:02:46.093880Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n"
                },
                {
                    "data": {
                        "text/plain": "GridSearchCV(cv=KFold(n_splits=3, random_state=72018, shuffle=True),\n       error_score='raise-deprecating',\n       estimator=Pipeline(memory=None,\n     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('polynomial_features', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('ridge_regression', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n   normalize=False, random_state=None, solver='auto', tol=0.001))]),\n       fit_params=None, iid='warn', n_jobs=None,\n       param_grid={'polynomial_features__degree': [1, 2, 3], 'ridge_regression__alpha': array([ 4.     ,  4.22827,  4.46956,  4.72462,  4.99424,  5.27925,\n        5.58052,  5.89898,  6.23562,  6.59146,  6.96761,  7.36523,\n        7.78554,  8.22984,  8.69949,  9.19594,  9.72072, 10.27545,\n       10.86184, 11.48169, 12.13691, 12.82953, 13.56167, 14.33559,\n       15.15367, 16.01844, 16.93257, 17.89885, 18.92028, 20.     ])},\n       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n       scoring=None, verbose=0)"
                    },
                    "execution_count": 38,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "grid.fit(X, y)"
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T18:03:07.016198Z",
                    "start_time": "2019-02-19T18:03:07.010215Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "(0.8504975378207537,\n {'polynomial_features__degree': 2,\n  'ridge_regression__alpha': 15.153673507519274})"
                    },
                    "execution_count": 39,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "grid.best_score_, grid.best_params_"
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T18:03:31.735568Z",
                    "start_time": "2019-02-19T18:03:31.728658Z"
                }
            },
            "outputs": [],
            "source": "y_predict = grid.predict(X)"
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T18:04:17.838943Z",
                    "start_time": "2019-02-19T18:04:17.832872Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.9149145594213683"
                    },
                    "execution_count": 41,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# This includes both in-sample and out-of-sample\nr2_score(y, y_predict)"
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T18:04:27.088854Z",
                    "start_time": "2019-02-19T18:04:27.082915Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([ 0.00000000e+00, -1.27346408e-01, -6.16205046e-03,  2.36135244e-02,\n        1.00398027e-01, -9.74110586e-01,  3.26236441e+00, -9.65057238e-01,\n       -1.96344725e+00,  8.56769182e-01, -1.01488960e+00, -7.06985966e-01,\n        5.52029222e-01, -3.03254502e+00,  7.74127927e-02,  7.24276605e-02,\n        6.82776638e-02,  1.72849044e+00, -4.80758341e-01,  5.76219972e-01,\n        1.28132069e-01,  2.22931335e-01, -7.45243542e-01,  1.66582495e-01,\n       -8.00025634e-02, -8.54571642e-02,  5.07490801e-01,  2.14820391e-01,\n       -1.48833274e-01,  1.42098626e-01,  1.93770221e-01,  5.02304885e-02,\n       -1.12667821e-01, -2.77559685e-01, -1.32870713e-01,  7.32239658e-01,\n        5.26857333e-02,  8.89966580e-02, -2.72228558e-01,  5.84383917e-01,\n        1.06306947e-01,  9.62971619e-01,  5.76845132e-01,  5.33378179e-01,\n        7.07913980e-01, -6.21760626e-02,  7.57641545e-02, -4.28157866e-01,\n        2.40651011e-01, -6.82201736e-01,  3.40931549e-01, -9.62217889e-01,\n       -8.14997204e-01,  2.81353294e-01,  5.50023518e-02,  8.65917517e-02,\n        6.28285056e-01, -1.40764851e-01, -1.03645734e-01, -3.81965497e-01,\n       -4.48817407e-01, -4.46562934e-01, -4.97293983e-01,  7.52862844e-01,\n       -8.00745322e-01,  7.86779267e-02, -5.78298566e-01, -4.98398516e-02,\n        5.37001246e-01,  2.24913740e-01, -7.11059542e-01,  5.70498060e-02,\n       -7.85214394e-01, -9.18516132e-01, -1.02907666e+00, -1.58937491e-01,\n       -7.77699453e-01,  1.42895792e-01,  7.72299871e-02,  1.08239035e+00,\n        3.98859145e-02, -7.26596891e-02, -9.64695031e-01, -1.12682105e+00,\n        1.01829108e+00, -6.12786851e-01, -4.22714073e-01, -1.41672983e-01,\n       -2.68672373e-01,  8.23071041e-01, -8.66106901e-01,  8.83695240e-01,\n        3.63975663e-01, -1.13200717e-01, -1.12043738e+00,  2.19170412e-03,\n        1.30087563e+00, -3.65505003e-01, -1.08425883e+00, -1.16852284e-01,\n        8.62081670e-02,  1.40937541e-03, -3.62535906e-01, -4.04519520e-01,\n        8.07960994e-01])"
                    },
                    "execution_count": 42,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# Notice that \"grid\" is a fit object!\n# We can use grid.predict(X_test) to get brand new predictions!\ngrid.best_estimator_.named_steps['ridge_regression'].coef_"
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-02-19T18:05:34.756588Z",
                    "start_time": "2019-02-19T18:05:34.728508Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n  warnings.warn(*warn_args, **warn_kwargs)\n/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n  warnings.warn(*warn_args, **warn_kwargs)\n/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n  warnings.warn(*warn_args, **warn_kwargs)\n/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n  warnings.warn(*warn_args, **warn_kwargs)\n/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n  warnings.warn(*warn_args, **warn_kwargs)\n"
                },
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_polynomial_features__degree</th>\n      <th>param_ridge_regression__alpha</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n      <th>split0_train_score</th>\n      <th>split1_train_score</th>\n      <th>split2_train_score</th>\n      <th>mean_train_score</th>\n      <th>std_train_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.004738</td>\n      <td>0.000674</td>\n      <td>0.004324</td>\n      <td>0.003960</td>\n      <td>1</td>\n      <td>4</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.672111</td>\n      <td>0.748235</td>\n      <td>0.701801</td>\n      <td>0.707393</td>\n      <td>0.031357</td>\n      <td>60</td>\n      <td>0.765659</td>\n      <td>0.732051</td>\n      <td>0.743390</td>\n      <td>0.747033</td>\n      <td>0.013960</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.004312</td>\n      <td>0.000122</td>\n      <td>0.001523</td>\n      <td>0.000010</td>\n      <td>1</td>\n      <td>4.22827</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.672103</td>\n      <td>0.748207</td>\n      <td>0.701986</td>\n      <td>0.707443</td>\n      <td>0.031337</td>\n      <td>59</td>\n      <td>0.765622</td>\n      <td>0.732017</td>\n      <td>0.743345</td>\n      <td>0.746994</td>\n      <td>0.013960</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.004194</td>\n      <td>0.000014</td>\n      <td>0.001516</td>\n      <td>0.000004</td>\n      <td>1</td>\n      <td>4.46956</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.672093</td>\n      <td>0.748175</td>\n      <td>0.702178</td>\n      <td>0.707493</td>\n      <td>0.031316</td>\n      <td>58</td>\n      <td>0.765582</td>\n      <td>0.731979</td>\n      <td>0.743295</td>\n      <td>0.746952</td>\n      <td>0.013960</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.004199</td>\n      <td>0.000018</td>\n      <td>0.001569</td>\n      <td>0.000005</td>\n      <td>1</td>\n      <td>4.72462</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.672081</td>\n      <td>0.748141</td>\n      <td>0.702375</td>\n      <td>0.707543</td>\n      <td>0.031295</td>\n      <td>57</td>\n      <td>0.765538</td>\n      <td>0.731938</td>\n      <td>0.743240</td>\n      <td>0.746905</td>\n      <td>0.013960</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.004204</td>\n      <td>0.000013</td>\n      <td>0.001580</td>\n      <td>0.000010</td>\n      <td>1</td>\n      <td>4.99424</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.672067</td>\n      <td>0.748104</td>\n      <td>0.702579</td>\n      <td>0.707593</td>\n      <td>0.031273</td>\n      <td>56</td>\n      <td>0.765489</td>\n      <td>0.731894</td>\n      <td>0.743180</td>\n      <td>0.746854</td>\n      <td>0.013959</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.004211</td>\n      <td>0.000046</td>\n      <td>0.001526</td>\n      <td>0.000007</td>\n      <td>1</td>\n      <td>5.27925</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.672050</td>\n      <td>0.748063</td>\n      <td>0.702788</td>\n      <td>0.707643</td>\n      <td>0.031251</td>\n      <td>55</td>\n      <td>0.765437</td>\n      <td>0.731844</td>\n      <td>0.743115</td>\n      <td>0.746799</td>\n      <td>0.013959</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.004189</td>\n      <td>0.000005</td>\n      <td>0.001525</td>\n      <td>0.000007</td>\n      <td>1</td>\n      <td>5.58052</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.672030</td>\n      <td>0.748018</td>\n      <td>0.703003</td>\n      <td>0.707693</td>\n      <td>0.031228</td>\n      <td>54</td>\n      <td>0.765379</td>\n      <td>0.731791</td>\n      <td>0.743043</td>\n      <td>0.746738</td>\n      <td>0.013959</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.004220</td>\n      <td>0.000010</td>\n      <td>0.001526</td>\n      <td>0.000010</td>\n      <td>1</td>\n      <td>5.89898</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.672007</td>\n      <td>0.747969</td>\n      <td>0.703223</td>\n      <td>0.707742</td>\n      <td>0.031205</td>\n      <td>53</td>\n      <td>0.765316</td>\n      <td>0.731732</td>\n      <td>0.742966</td>\n      <td>0.746671</td>\n      <td>0.013959</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.004239</td>\n      <td>0.000013</td>\n      <td>0.001562</td>\n      <td>0.000030</td>\n      <td>1</td>\n      <td>6.23562</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.671980</td>\n      <td>0.747915</td>\n      <td>0.703449</td>\n      <td>0.707790</td>\n      <td>0.031181</td>\n      <td>52</td>\n      <td>0.765248</td>\n      <td>0.731668</td>\n      <td>0.742881</td>\n      <td>0.746599</td>\n      <td>0.013959</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.004358</td>\n      <td>0.000115</td>\n      <td>0.001578</td>\n      <td>0.000027</td>\n      <td>1</td>\n      <td>6.59146</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.671950</td>\n      <td>0.747856</td>\n      <td>0.703679</td>\n      <td>0.707836</td>\n      <td>0.031157</td>\n      <td>50</td>\n      <td>0.765174</td>\n      <td>0.731598</td>\n      <td>0.742789</td>\n      <td>0.746520</td>\n      <td>0.013959</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.004259</td>\n      <td>0.000018</td>\n      <td>0.001536</td>\n      <td>0.000007</td>\n      <td>1</td>\n      <td>6.96761</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.671915</td>\n      <td>0.747791</td>\n      <td>0.703915</td>\n      <td>0.707882</td>\n      <td>0.031133</td>\n      <td>48</td>\n      <td>0.765093</td>\n      <td>0.731521</td>\n      <td>0.742689</td>\n      <td>0.746434</td>\n      <td>0.013959</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.007119</td>\n      <td>0.003928</td>\n      <td>0.001588</td>\n      <td>0.000085</td>\n      <td>1</td>\n      <td>7.36523</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.671877</td>\n      <td>0.747721</td>\n      <td>0.704155</td>\n      <td>0.707925</td>\n      <td>0.031108</td>\n      <td>47</td>\n      <td>0.765005</td>\n      <td>0.731438</td>\n      <td>0.742580</td>\n      <td>0.746341</td>\n      <td>0.013959</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.005207</td>\n      <td>0.000434</td>\n      <td>0.001854</td>\n      <td>0.000214</td>\n      <td>1</td>\n      <td>7.78554</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.671833</td>\n      <td>0.747645</td>\n      <td>0.704400</td>\n      <td>0.707966</td>\n      <td>0.031082</td>\n      <td>45</td>\n      <td>0.764910</td>\n      <td>0.731348</td>\n      <td>0.742462</td>\n      <td>0.746240</td>\n      <td>0.013960</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.004516</td>\n      <td>0.000158</td>\n      <td>0.001600</td>\n      <td>0.000093</td>\n      <td>1</td>\n      <td>8.22984</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.671785</td>\n      <td>0.747561</td>\n      <td>0.704648</td>\n      <td>0.708005</td>\n      <td>0.031056</td>\n      <td>44</td>\n      <td>0.764807</td>\n      <td>0.731249</td>\n      <td>0.742335</td>\n      <td>0.746130</td>\n      <td>0.013960</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.005064</td>\n      <td>0.000913</td>\n      <td>0.001545</td>\n      <td>0.000018</td>\n      <td>1</td>\n      <td>8.69949</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.671731</td>\n      <td>0.747471</td>\n      <td>0.704900</td>\n      <td>0.708040</td>\n      <td>0.031030</td>\n      <td>42</td>\n      <td>0.764695</td>\n      <td>0.731142</td>\n      <td>0.742196</td>\n      <td>0.746011</td>\n      <td>0.013961</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.004299</td>\n      <td>0.000018</td>\n      <td>0.001571</td>\n      <td>0.000052</td>\n      <td>1</td>\n      <td>9.19594</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.671671</td>\n      <td>0.747372</td>\n      <td>0.705155</td>\n      <td>0.708072</td>\n      <td>0.031003</td>\n      <td>40</td>\n      <td>0.764574</td>\n      <td>0.731026</td>\n      <td>0.742047</td>\n      <td>0.745883</td>\n      <td>0.013962</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.004216</td>\n      <td>0.000018</td>\n      <td>0.001589</td>\n      <td>0.000059</td>\n      <td>1</td>\n      <td>9.72072</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.671606</td>\n      <td>0.747265</td>\n      <td>0.705413</td>\n      <td>0.708100</td>\n      <td>0.030976</td>\n      <td>39</td>\n      <td>0.764444</td>\n      <td>0.730900</td>\n      <td>0.741886</td>\n      <td>0.745743</td>\n      <td>0.013963</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.004230</td>\n      <td>0.000029</td>\n      <td>0.001522</td>\n      <td>0.000005</td>\n      <td>1</td>\n      <td>10.2755</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.671533</td>\n      <td>0.747149</td>\n      <td>0.705672</td>\n      <td>0.708123</td>\n      <td>0.030949</td>\n      <td>37</td>\n      <td>0.764303</td>\n      <td>0.730764</td>\n      <td>0.741712</td>\n      <td>0.745593</td>\n      <td>0.013965</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.004297</td>\n      <td>0.000078</td>\n      <td>0.001521</td>\n      <td>0.000013</td>\n      <td>1</td>\n      <td>10.8618</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.671453</td>\n      <td>0.747022</td>\n      <td>0.705933</td>\n      <td>0.708141</td>\n      <td>0.030921</td>\n      <td>35</td>\n      <td>0.764151</td>\n      <td>0.730616</td>\n      <td>0.741524</td>\n      <td>0.745431</td>\n      <td>0.013966</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.004201</td>\n      <td>0.000010</td>\n      <td>0.001564</td>\n      <td>0.000041</td>\n      <td>1</td>\n      <td>11.4817</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.671366</td>\n      <td>0.746886</td>\n      <td>0.706195</td>\n      <td>0.708153</td>\n      <td>0.030892</td>\n      <td>33</td>\n      <td>0.763987</td>\n      <td>0.730457</td>\n      <td>0.741322</td>\n      <td>0.745255</td>\n      <td>0.013968</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.004248</td>\n      <td>0.000083</td>\n      <td>0.001667</td>\n      <td>0.000108</td>\n      <td>1</td>\n      <td>12.1369</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.671271</td>\n      <td>0.746738</td>\n      <td>0.706456</td>\n      <td>0.708159</td>\n      <td>0.030863</td>\n      <td>31</td>\n      <td>0.763811</td>\n      <td>0.730284</td>\n      <td>0.741105</td>\n      <td>0.745067</td>\n      <td>0.013971</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.004488</td>\n      <td>0.000377</td>\n      <td>0.001601</td>\n      <td>0.000125</td>\n      <td>1</td>\n      <td>12.8295</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.671168</td>\n      <td>0.746578</td>\n      <td>0.706717</td>\n      <td>0.708157</td>\n      <td>0.030833</td>\n      <td>32</td>\n      <td>0.763622</td>\n      <td>0.730099</td>\n      <td>0.740872</td>\n      <td>0.744864</td>\n      <td>0.013974</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.004185</td>\n      <td>0.000011</td>\n      <td>0.001562</td>\n      <td>0.000049</td>\n      <td>1</td>\n      <td>13.5617</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.671056</td>\n      <td>0.746405</td>\n      <td>0.706975</td>\n      <td>0.708148</td>\n      <td>0.030803</td>\n      <td>34</td>\n      <td>0.763418</td>\n      <td>0.729898</td>\n      <td>0.740622</td>\n      <td>0.744646</td>\n      <td>0.013977</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.004231</td>\n      <td>0.000014</td>\n      <td>0.001524</td>\n      <td>0.000023</td>\n      <td>1</td>\n      <td>14.3356</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.670934</td>\n      <td>0.746219</td>\n      <td>0.707232</td>\n      <td>0.708130</td>\n      <td>0.030772</td>\n      <td>36</td>\n      <td>0.763200</td>\n      <td>0.729683</td>\n      <td>0.740354</td>\n      <td>0.744412</td>\n      <td>0.013981</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.004183</td>\n      <td>0.000010</td>\n      <td>0.001515</td>\n      <td>0.000009</td>\n      <td>1</td>\n      <td>15.1537</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.670802</td>\n      <td>0.746018</td>\n      <td>0.707484</td>\n      <td>0.708103</td>\n      <td>0.030740</td>\n      <td>38</td>\n      <td>0.762965</td>\n      <td>0.729451</td>\n      <td>0.740066</td>\n      <td>0.744161</td>\n      <td>0.013985</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.004175</td>\n      <td>0.000014</td>\n      <td>0.001584</td>\n      <td>0.000026</td>\n      <td>1</td>\n      <td>16.0184</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.670660</td>\n      <td>0.745801</td>\n      <td>0.707732</td>\n      <td>0.708065</td>\n      <td>0.030708</td>\n      <td>41</td>\n      <td>0.762714</td>\n      <td>0.729202</td>\n      <td>0.739759</td>\n      <td>0.743892</td>\n      <td>0.013990</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.004168</td>\n      <td>0.000016</td>\n      <td>0.001584</td>\n      <td>0.000012</td>\n      <td>1</td>\n      <td>16.9326</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.670506</td>\n      <td>0.745568</td>\n      <td>0.707974</td>\n      <td>0.708016</td>\n      <td>0.030674</td>\n      <td>43</td>\n      <td>0.762446</td>\n      <td>0.728934</td>\n      <td>0.739430</td>\n      <td>0.743603</td>\n      <td>0.013996</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.004211</td>\n      <td>0.000007</td>\n      <td>0.001540</td>\n      <td>0.000037</td>\n      <td>1</td>\n      <td>17.8989</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.670341</td>\n      <td>0.745317</td>\n      <td>0.708208</td>\n      <td>0.707955</td>\n      <td>0.030639</td>\n      <td>46</td>\n      <td>0.762158</td>\n      <td>0.728647</td>\n      <td>0.739080</td>\n      <td>0.743295</td>\n      <td>0.014002</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.004292</td>\n      <td>0.000033</td>\n      <td>0.001565</td>\n      <td>0.000054</td>\n      <td>1</td>\n      <td>18.9203</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.670164</td>\n      <td>0.745048</td>\n      <td>0.708435</td>\n      <td>0.707881</td>\n      <td>0.030604</td>\n      <td>49</td>\n      <td>0.761851</td>\n      <td>0.728339</td>\n      <td>0.738705</td>\n      <td>0.742965</td>\n      <td>0.014009</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.004288</td>\n      <td>0.000017</td>\n      <td>0.001613</td>\n      <td>0.000021</td>\n      <td>1</td>\n      <td>20</td>\n      <td>{'polynomial_features__degree': 1, 'ridge_regr...</td>\n      <td>0.669974</td>\n      <td>0.744758</td>\n      <td>0.708651</td>\n      <td>0.707793</td>\n      <td>0.030566</td>\n      <td>51</td>\n      <td>0.761523</td>\n      <td>0.728010</td>\n      <td>0.738306</td>\n      <td>0.742613</td>\n      <td>0.014017</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>0.020037</td>\n      <td>0.000911</td>\n      <td>0.010517</td>\n      <td>0.001911</td>\n      <td>3</td>\n      <td>4</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.460323</td>\n      <td>0.622404</td>\n      <td>0.196931</td>\n      <td>0.427007</td>\n      <td>0.175208</td>\n      <td>90</td>\n      <td>0.976990</td>\n      <td>0.983499</td>\n      <td>0.977697</td>\n      <td>0.979396</td>\n      <td>0.002916</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>0.019005</td>\n      <td>0.000515</td>\n      <td>0.008061</td>\n      <td>0.000011</td>\n      <td>3</td>\n      <td>4.22827</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.466577</td>\n      <td>0.634059</td>\n      <td>0.218160</td>\n      <td>0.440037</td>\n      <td>0.170743</td>\n      <td>89</td>\n      <td>0.976510</td>\n      <td>0.983157</td>\n      <td>0.977301</td>\n      <td>0.978989</td>\n      <td>0.002965</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>0.022073</td>\n      <td>0.004107</td>\n      <td>0.008206</td>\n      <td>0.000111</td>\n      <td>3</td>\n      <td>4.46956</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.472713</td>\n      <td>0.645280</td>\n      <td>0.238909</td>\n      <td>0.452722</td>\n      <td>0.166421</td>\n      <td>88</td>\n      <td>0.976021</td>\n      <td>0.982807</td>\n      <td>0.976897</td>\n      <td>0.978575</td>\n      <td>0.003014</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>0.018684</td>\n      <td>0.000259</td>\n      <td>0.008166</td>\n      <td>0.000047</td>\n      <td>3</td>\n      <td>4.72462</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.478741</td>\n      <td>0.656072</td>\n      <td>0.259172</td>\n      <td>0.465068</td>\n      <td>0.162242</td>\n      <td>87</td>\n      <td>0.975524</td>\n      <td>0.982451</td>\n      <td>0.976485</td>\n      <td>0.978153</td>\n      <td>0.003064</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>0.020220</td>\n      <td>0.001181</td>\n      <td>0.008266</td>\n      <td>0.000285</td>\n      <td>3</td>\n      <td>4.99424</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.484671</td>\n      <td>0.666443</td>\n      <td>0.278946</td>\n      <td>0.477078</td>\n      <td>0.158208</td>\n      <td>86</td>\n      <td>0.975019</td>\n      <td>0.982086</td>\n      <td>0.976065</td>\n      <td>0.977723</td>\n      <td>0.003115</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>0.018943</td>\n      <td>0.000081</td>\n      <td>0.008037</td>\n      <td>0.000006</td>\n      <td>3</td>\n      <td>5.27925</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.490512</td>\n      <td>0.676400</td>\n      <td>0.298230</td>\n      <td>0.488757</td>\n      <td>0.154315</td>\n      <td>85</td>\n      <td>0.974504</td>\n      <td>0.981714</td>\n      <td>0.975637</td>\n      <td>0.977285</td>\n      <td>0.003166</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>0.024548</td>\n      <td>0.008243</td>\n      <td>0.008494</td>\n      <td>0.000337</td>\n      <td>3</td>\n      <td>5.58052</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.496272</td>\n      <td>0.685952</td>\n      <td>0.317024</td>\n      <td>0.500110</td>\n      <td>0.150564</td>\n      <td>84</td>\n      <td>0.973981</td>\n      <td>0.981335</td>\n      <td>0.975200</td>\n      <td>0.976839</td>\n      <td>0.003218</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>0.021373</td>\n      <td>0.000728</td>\n      <td>0.008550</td>\n      <td>0.000348</td>\n      <td>3</td>\n      <td>5.89898</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.501957</td>\n      <td>0.695107</td>\n      <td>0.335328</td>\n      <td>0.511144</td>\n      <td>0.146950</td>\n      <td>83</td>\n      <td>0.973449</td>\n      <td>0.980948</td>\n      <td>0.974754</td>\n      <td>0.976384</td>\n      <td>0.003271</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>0.018997</td>\n      <td>0.000180</td>\n      <td>0.008299</td>\n      <td>0.000085</td>\n      <td>3</td>\n      <td>6.23562</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.507575</td>\n      <td>0.703872</td>\n      <td>0.353147</td>\n      <td>0.521864</td>\n      <td>0.143469</td>\n      <td>82</td>\n      <td>0.972908</td>\n      <td>0.980554</td>\n      <td>0.974299</td>\n      <td>0.975920</td>\n      <td>0.003325</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>0.020786</td>\n      <td>0.002413</td>\n      <td>0.009046</td>\n      <td>0.001268</td>\n      <td>3</td>\n      <td>6.59146</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.513131</td>\n      <td>0.712259</td>\n      <td>0.370484</td>\n      <td>0.532277</td>\n      <td>0.140117</td>\n      <td>81</td>\n      <td>0.972358</td>\n      <td>0.980152</td>\n      <td>0.973836</td>\n      <td>0.975449</td>\n      <td>0.003380</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>0.023526</td>\n      <td>0.002011</td>\n      <td>0.009093</td>\n      <td>0.000357</td>\n      <td>3</td>\n      <td>6.96761</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.518629</td>\n      <td>0.720275</td>\n      <td>0.387344</td>\n      <td>0.542388</td>\n      <td>0.136889</td>\n      <td>80</td>\n      <td>0.971800</td>\n      <td>0.979742</td>\n      <td>0.973363</td>\n      <td>0.974968</td>\n      <td>0.003435</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>0.020208</td>\n      <td>0.000556</td>\n      <td>0.008618</td>\n      <td>0.000454</td>\n      <td>3</td>\n      <td>7.36523</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.524073</td>\n      <td>0.727930</td>\n      <td>0.403733</td>\n      <td>0.552205</td>\n      <td>0.133778</td>\n      <td>79</td>\n      <td>0.971232</td>\n      <td>0.979324</td>\n      <td>0.972880</td>\n      <td>0.974479</td>\n      <td>0.003492</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>0.022621</td>\n      <td>0.001939</td>\n      <td>0.008801</td>\n      <td>0.000125</td>\n      <td>3</td>\n      <td>7.78554</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.529466</td>\n      <td>0.735234</td>\n      <td>0.419660</td>\n      <td>0.561734</td>\n      <td>0.130780</td>\n      <td>78</td>\n      <td>0.970655</td>\n      <td>0.978899</td>\n      <td>0.972388</td>\n      <td>0.973981</td>\n      <td>0.003549</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>0.021202</td>\n      <td>0.000132</td>\n      <td>0.008622</td>\n      <td>0.000095</td>\n      <td>3</td>\n      <td>8.22984</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.534811</td>\n      <td>0.742198</td>\n      <td>0.435131</td>\n      <td>0.570981</td>\n      <td>0.127889</td>\n      <td>77</td>\n      <td>0.970069</td>\n      <td>0.978466</td>\n      <td>0.971887</td>\n      <td>0.973474</td>\n      <td>0.003607</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>0.020939</td>\n      <td>0.000074</td>\n      <td>0.008663</td>\n      <td>0.000035</td>\n      <td>3</td>\n      <td>8.69949</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.540109</td>\n      <td>0.748831</td>\n      <td>0.450155</td>\n      <td>0.579954</td>\n      <td>0.125097</td>\n      <td>76</td>\n      <td>0.969474</td>\n      <td>0.978025</td>\n      <td>0.971376</td>\n      <td>0.972958</td>\n      <td>0.003666</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>0.021338</td>\n      <td>0.000309</td>\n      <td>0.008691</td>\n      <td>0.000119</td>\n      <td>3</td>\n      <td>9.19594</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.545361</td>\n      <td>0.755144</td>\n      <td>0.464741</td>\n      <td>0.588660</td>\n      <td>0.122400</td>\n      <td>75</td>\n      <td>0.968870</td>\n      <td>0.977576</td>\n      <td>0.970854</td>\n      <td>0.972434</td>\n      <td>0.003725</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>0.021102</td>\n      <td>0.000270</td>\n      <td>0.008800</td>\n      <td>0.000271</td>\n      <td>3</td>\n      <td>9.72072</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.550568</td>\n      <td>0.761148</td>\n      <td>0.478899</td>\n      <td>0.597104</td>\n      <td>0.119792</td>\n      <td>74</td>\n      <td>0.968257</td>\n      <td>0.977119</td>\n      <td>0.970323</td>\n      <td>0.971900</td>\n      <td>0.003786</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>0.020766</td>\n      <td>0.000870</td>\n      <td>0.008658</td>\n      <td>0.000458</td>\n      <td>3</td>\n      <td>10.2755</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.555728</td>\n      <td>0.766852</td>\n      <td>0.492638</td>\n      <td>0.605295</td>\n      <td>0.117268</td>\n      <td>73</td>\n      <td>0.967635</td>\n      <td>0.976654</td>\n      <td>0.969783</td>\n      <td>0.971357</td>\n      <td>0.003847</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>0.019286</td>\n      <td>0.000211</td>\n      <td>0.008466</td>\n      <td>0.000154</td>\n      <td>3</td>\n      <td>10.8618</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.560842</td>\n      <td>0.772267</td>\n      <td>0.505970</td>\n      <td>0.613238</td>\n      <td>0.114821</td>\n      <td>72</td>\n      <td>0.967004</td>\n      <td>0.976182</td>\n      <td>0.969232</td>\n      <td>0.970806</td>\n      <td>0.003909</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>0.026452</td>\n      <td>0.003622</td>\n      <td>0.009195</td>\n      <td>0.000279</td>\n      <td>3</td>\n      <td>11.4817</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.565909</td>\n      <td>0.777404</td>\n      <td>0.518904</td>\n      <td>0.620940</td>\n      <td>0.112449</td>\n      <td>71</td>\n      <td>0.966364</td>\n      <td>0.975701</td>\n      <td>0.968671</td>\n      <td>0.970246</td>\n      <td>0.003971</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>0.021899</td>\n      <td>0.001211</td>\n      <td>0.008698</td>\n      <td>0.000266</td>\n      <td>3</td>\n      <td>12.1369</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.570925</td>\n      <td>0.782273</td>\n      <td>0.531450</td>\n      <td>0.628407</td>\n      <td>0.110145</td>\n      <td>70</td>\n      <td>0.965716</td>\n      <td>0.975213</td>\n      <td>0.968100</td>\n      <td>0.969676</td>\n      <td>0.004034</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>0.021542</td>\n      <td>0.000339</td>\n      <td>0.008793</td>\n      <td>0.000244</td>\n      <td>3</td>\n      <td>12.8295</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.575891</td>\n      <td>0.786885</td>\n      <td>0.543618</td>\n      <td>0.635646</td>\n      <td>0.107907</td>\n      <td>69</td>\n      <td>0.965059</td>\n      <td>0.974717</td>\n      <td>0.967519</td>\n      <td>0.969098</td>\n      <td>0.004098</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>0.021657</td>\n      <td>0.000230</td>\n      <td>0.008949</td>\n      <td>0.000086</td>\n      <td>3</td>\n      <td>13.5617</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.580802</td>\n      <td>0.791249</td>\n      <td>0.555420</td>\n      <td>0.642663</td>\n      <td>0.105731</td>\n      <td>68</td>\n      <td>0.964393</td>\n      <td>0.974213</td>\n      <td>0.966928</td>\n      <td>0.968511</td>\n      <td>0.004162</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>0.021905</td>\n      <td>0.000758</td>\n      <td>0.008883</td>\n      <td>0.000189</td>\n      <td>3</td>\n      <td>14.3356</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.585658</td>\n      <td>0.795377</td>\n      <td>0.566865</td>\n      <td>0.649463</td>\n      <td>0.103614</td>\n      <td>67</td>\n      <td>0.963718</td>\n      <td>0.973700</td>\n      <td>0.966327</td>\n      <td>0.967915</td>\n      <td>0.004227</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>0.021386</td>\n      <td>0.000259</td>\n      <td>0.009050</td>\n      <td>0.000517</td>\n      <td>3</td>\n      <td>15.1537</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.590454</td>\n      <td>0.799277</td>\n      <td>0.577963</td>\n      <td>0.656052</td>\n      <td>0.101553</td>\n      <td>66</td>\n      <td>0.963036</td>\n      <td>0.973180</td>\n      <td>0.965717</td>\n      <td>0.967311</td>\n      <td>0.004292</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>0.021089</td>\n      <td>0.000108</td>\n      <td>0.008622</td>\n      <td>0.000029</td>\n      <td>3</td>\n      <td>16.0184</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.595188</td>\n      <td>0.802959</td>\n      <td>0.588724</td>\n      <td>0.662436</td>\n      <td>0.099548</td>\n      <td>65</td>\n      <td>0.962345</td>\n      <td>0.972652</td>\n      <td>0.965096</td>\n      <td>0.966698</td>\n      <td>0.004358</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>0.021909</td>\n      <td>0.000889</td>\n      <td>0.009245</td>\n      <td>0.000663</td>\n      <td>3</td>\n      <td>16.9326</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.599857</td>\n      <td>0.806434</td>\n      <td>0.599156</td>\n      <td>0.668619</td>\n      <td>0.097594</td>\n      <td>64</td>\n      <td>0.961647</td>\n      <td>0.972117</td>\n      <td>0.964466</td>\n      <td>0.966077</td>\n      <td>0.004423</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>0.021734</td>\n      <td>0.000326</td>\n      <td>0.008887</td>\n      <td>0.000068</td>\n      <td>3</td>\n      <td>17.8989</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.604457</td>\n      <td>0.809709</td>\n      <td>0.609271</td>\n      <td>0.674608</td>\n      <td>0.095693</td>\n      <td>63</td>\n      <td>0.960941</td>\n      <td>0.971573</td>\n      <td>0.963827</td>\n      <td>0.965447</td>\n      <td>0.004489</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>0.022268</td>\n      <td>0.000916</td>\n      <td>0.009336</td>\n      <td>0.000745</td>\n      <td>3</td>\n      <td>18.9203</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.608986</td>\n      <td>0.812795</td>\n      <td>0.619075</td>\n      <td>0.680407</td>\n      <td>0.093842</td>\n      <td>62</td>\n      <td>0.960227</td>\n      <td>0.971021</td>\n      <td>0.963177</td>\n      <td>0.964809</td>\n      <td>0.004555</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>0.021807</td>\n      <td>0.001021</td>\n      <td>0.008908</td>\n      <td>0.000091</td>\n      <td>3</td>\n      <td>20</td>\n      <td>{'polynomial_features__degree': 3, 'ridge_regr...</td>\n      <td>0.613441</td>\n      <td>0.815700</td>\n      <td>0.628578</td>\n      <td>0.686020</td>\n      <td>0.092041</td>\n      <td>61</td>\n      <td>0.959506</td>\n      <td>0.970462</td>\n      <td>0.962519</td>\n      <td>0.964162</td>\n      <td>0.004621</td>\n    </tr>\n  </tbody>\n</table>\n<p>90 rows \u00d7 18 columns</p>\n</div>",
                        "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        0.004738      0.000674         0.004324        0.003960   \n1        0.004312      0.000122         0.001523        0.000010   \n2        0.004194      0.000014         0.001516        0.000004   \n3        0.004199      0.000018         0.001569        0.000005   \n4        0.004204      0.000013         0.001580        0.000010   \n5        0.004211      0.000046         0.001526        0.000007   \n6        0.004189      0.000005         0.001525        0.000007   \n7        0.004220      0.000010         0.001526        0.000010   \n8        0.004239      0.000013         0.001562        0.000030   \n9        0.004358      0.000115         0.001578        0.000027   \n10       0.004259      0.000018         0.001536        0.000007   \n11       0.007119      0.003928         0.001588        0.000085   \n12       0.005207      0.000434         0.001854        0.000214   \n13       0.004516      0.000158         0.001600        0.000093   \n14       0.005064      0.000913         0.001545        0.000018   \n15       0.004299      0.000018         0.001571        0.000052   \n16       0.004216      0.000018         0.001589        0.000059   \n17       0.004230      0.000029         0.001522        0.000005   \n18       0.004297      0.000078         0.001521        0.000013   \n19       0.004201      0.000010         0.001564        0.000041   \n20       0.004248      0.000083         0.001667        0.000108   \n21       0.004488      0.000377         0.001601        0.000125   \n22       0.004185      0.000011         0.001562        0.000049   \n23       0.004231      0.000014         0.001524        0.000023   \n24       0.004183      0.000010         0.001515        0.000009   \n25       0.004175      0.000014         0.001584        0.000026   \n26       0.004168      0.000016         0.001584        0.000012   \n27       0.004211      0.000007         0.001540        0.000037   \n28       0.004292      0.000033         0.001565        0.000054   \n29       0.004288      0.000017         0.001613        0.000021   \n..            ...           ...              ...             ...   \n60       0.020037      0.000911         0.010517        0.001911   \n61       0.019005      0.000515         0.008061        0.000011   \n62       0.022073      0.004107         0.008206        0.000111   \n63       0.018684      0.000259         0.008166        0.000047   \n64       0.020220      0.001181         0.008266        0.000285   \n65       0.018943      0.000081         0.008037        0.000006   \n66       0.024548      0.008243         0.008494        0.000337   \n67       0.021373      0.000728         0.008550        0.000348   \n68       0.018997      0.000180         0.008299        0.000085   \n69       0.020786      0.002413         0.009046        0.001268   \n70       0.023526      0.002011         0.009093        0.000357   \n71       0.020208      0.000556         0.008618        0.000454   \n72       0.022621      0.001939         0.008801        0.000125   \n73       0.021202      0.000132         0.008622        0.000095   \n74       0.020939      0.000074         0.008663        0.000035   \n75       0.021338      0.000309         0.008691        0.000119   \n76       0.021102      0.000270         0.008800        0.000271   \n77       0.020766      0.000870         0.008658        0.000458   \n78       0.019286      0.000211         0.008466        0.000154   \n79       0.026452      0.003622         0.009195        0.000279   \n80       0.021899      0.001211         0.008698        0.000266   \n81       0.021542      0.000339         0.008793        0.000244   \n82       0.021657      0.000230         0.008949        0.000086   \n83       0.021905      0.000758         0.008883        0.000189   \n84       0.021386      0.000259         0.009050        0.000517   \n85       0.021089      0.000108         0.008622        0.000029   \n86       0.021909      0.000889         0.009245        0.000663   \n87       0.021734      0.000326         0.008887        0.000068   \n88       0.022268      0.000916         0.009336        0.000745   \n89       0.021807      0.001021         0.008908        0.000091   \n\n   param_polynomial_features__degree param_ridge_regression__alpha  \\\n0                                  1                             4   \n1                                  1                       4.22827   \n2                                  1                       4.46956   \n3                                  1                       4.72462   \n4                                  1                       4.99424   \n5                                  1                       5.27925   \n6                                  1                       5.58052   \n7                                  1                       5.89898   \n8                                  1                       6.23562   \n9                                  1                       6.59146   \n10                                 1                       6.96761   \n11                                 1                       7.36523   \n12                                 1                       7.78554   \n13                                 1                       8.22984   \n14                                 1                       8.69949   \n15                                 1                       9.19594   \n16                                 1                       9.72072   \n17                                 1                       10.2755   \n18                                 1                       10.8618   \n19                                 1                       11.4817   \n20                                 1                       12.1369   \n21                                 1                       12.8295   \n22                                 1                       13.5617   \n23                                 1                       14.3356   \n24                                 1                       15.1537   \n25                                 1                       16.0184   \n26                                 1                       16.9326   \n27                                 1                       17.8989   \n28                                 1                       18.9203   \n29                                 1                            20   \n..                               ...                           ...   \n60                                 3                             4   \n61                                 3                       4.22827   \n62                                 3                       4.46956   \n63                                 3                       4.72462   \n64                                 3                       4.99424   \n65                                 3                       5.27925   \n66                                 3                       5.58052   \n67                                 3                       5.89898   \n68                                 3                       6.23562   \n69                                 3                       6.59146   \n70                                 3                       6.96761   \n71                                 3                       7.36523   \n72                                 3                       7.78554   \n73                                 3                       8.22984   \n74                                 3                       8.69949   \n75                                 3                       9.19594   \n76                                 3                       9.72072   \n77                                 3                       10.2755   \n78                                 3                       10.8618   \n79                                 3                       11.4817   \n80                                 3                       12.1369   \n81                                 3                       12.8295   \n82                                 3                       13.5617   \n83                                 3                       14.3356   \n84                                 3                       15.1537   \n85                                 3                       16.0184   \n86                                 3                       16.9326   \n87                                 3                       17.8989   \n88                                 3                       18.9203   \n89                                 3                            20   \n\n                                               params  split0_test_score  \\\n0   {'polynomial_features__degree': 1, 'ridge_regr...           0.672111   \n1   {'polynomial_features__degree': 1, 'ridge_regr...           0.672103   \n2   {'polynomial_features__degree': 1, 'ridge_regr...           0.672093   \n3   {'polynomial_features__degree': 1, 'ridge_regr...           0.672081   \n4   {'polynomial_features__degree': 1, 'ridge_regr...           0.672067   \n5   {'polynomial_features__degree': 1, 'ridge_regr...           0.672050   \n6   {'polynomial_features__degree': 1, 'ridge_regr...           0.672030   \n7   {'polynomial_features__degree': 1, 'ridge_regr...           0.672007   \n8   {'polynomial_features__degree': 1, 'ridge_regr...           0.671980   \n9   {'polynomial_features__degree': 1, 'ridge_regr...           0.671950   \n10  {'polynomial_features__degree': 1, 'ridge_regr...           0.671915   \n11  {'polynomial_features__degree': 1, 'ridge_regr...           0.671877   \n12  {'polynomial_features__degree': 1, 'ridge_regr...           0.671833   \n13  {'polynomial_features__degree': 1, 'ridge_regr...           0.671785   \n14  {'polynomial_features__degree': 1, 'ridge_regr...           0.671731   \n15  {'polynomial_features__degree': 1, 'ridge_regr...           0.671671   \n16  {'polynomial_features__degree': 1, 'ridge_regr...           0.671606   \n17  {'polynomial_features__degree': 1, 'ridge_regr...           0.671533   \n18  {'polynomial_features__degree': 1, 'ridge_regr...           0.671453   \n19  {'polynomial_features__degree': 1, 'ridge_regr...           0.671366   \n20  {'polynomial_features__degree': 1, 'ridge_regr...           0.671271   \n21  {'polynomial_features__degree': 1, 'ridge_regr...           0.671168   \n22  {'polynomial_features__degree': 1, 'ridge_regr...           0.671056   \n23  {'polynomial_features__degree': 1, 'ridge_regr...           0.670934   \n24  {'polynomial_features__degree': 1, 'ridge_regr...           0.670802   \n25  {'polynomial_features__degree': 1, 'ridge_regr...           0.670660   \n26  {'polynomial_features__degree': 1, 'ridge_regr...           0.670506   \n27  {'polynomial_features__degree': 1, 'ridge_regr...           0.670341   \n28  {'polynomial_features__degree': 1, 'ridge_regr...           0.670164   \n29  {'polynomial_features__degree': 1, 'ridge_regr...           0.669974   \n..                                                ...                ...   \n60  {'polynomial_features__degree': 3, 'ridge_regr...           0.460323   \n61  {'polynomial_features__degree': 3, 'ridge_regr...           0.466577   \n62  {'polynomial_features__degree': 3, 'ridge_regr...           0.472713   \n63  {'polynomial_features__degree': 3, 'ridge_regr...           0.478741   \n64  {'polynomial_features__degree': 3, 'ridge_regr...           0.484671   \n65  {'polynomial_features__degree': 3, 'ridge_regr...           0.490512   \n66  {'polynomial_features__degree': 3, 'ridge_regr...           0.496272   \n67  {'polynomial_features__degree': 3, 'ridge_regr...           0.501957   \n68  {'polynomial_features__degree': 3, 'ridge_regr...           0.507575   \n69  {'polynomial_features__degree': 3, 'ridge_regr...           0.513131   \n70  {'polynomial_features__degree': 3, 'ridge_regr...           0.518629   \n71  {'polynomial_features__degree': 3, 'ridge_regr...           0.524073   \n72  {'polynomial_features__degree': 3, 'ridge_regr...           0.529466   \n73  {'polynomial_features__degree': 3, 'ridge_regr...           0.534811   \n74  {'polynomial_features__degree': 3, 'ridge_regr...           0.540109   \n75  {'polynomial_features__degree': 3, 'ridge_regr...           0.545361   \n76  {'polynomial_features__degree': 3, 'ridge_regr...           0.550568   \n77  {'polynomial_features__degree': 3, 'ridge_regr...           0.555728   \n78  {'polynomial_features__degree': 3, 'ridge_regr...           0.560842   \n79  {'polynomial_features__degree': 3, 'ridge_regr...           0.565909   \n80  {'polynomial_features__degree': 3, 'ridge_regr...           0.570925   \n81  {'polynomial_features__degree': 3, 'ridge_regr...           0.575891   \n82  {'polynomial_features__degree': 3, 'ridge_regr...           0.580802   \n83  {'polynomial_features__degree': 3, 'ridge_regr...           0.585658   \n84  {'polynomial_features__degree': 3, 'ridge_regr...           0.590454   \n85  {'polynomial_features__degree': 3, 'ridge_regr...           0.595188   \n86  {'polynomial_features__degree': 3, 'ridge_regr...           0.599857   \n87  {'polynomial_features__degree': 3, 'ridge_regr...           0.604457   \n88  {'polynomial_features__degree': 3, 'ridge_regr...           0.608986   \n89  {'polynomial_features__degree': 3, 'ridge_regr...           0.613441   \n\n    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n0            0.748235           0.701801         0.707393        0.031357   \n1            0.748207           0.701986         0.707443        0.031337   \n2            0.748175           0.702178         0.707493        0.031316   \n3            0.748141           0.702375         0.707543        0.031295   \n4            0.748104           0.702579         0.707593        0.031273   \n5            0.748063           0.702788         0.707643        0.031251   \n6            0.748018           0.703003         0.707693        0.031228   \n7            0.747969           0.703223         0.707742        0.031205   \n8            0.747915           0.703449         0.707790        0.031181   \n9            0.747856           0.703679         0.707836        0.031157   \n10           0.747791           0.703915         0.707882        0.031133   \n11           0.747721           0.704155         0.707925        0.031108   \n12           0.747645           0.704400         0.707966        0.031082   \n13           0.747561           0.704648         0.708005        0.031056   \n14           0.747471           0.704900         0.708040        0.031030   \n15           0.747372           0.705155         0.708072        0.031003   \n16           0.747265           0.705413         0.708100        0.030976   \n17           0.747149           0.705672         0.708123        0.030949   \n18           0.747022           0.705933         0.708141        0.030921   \n19           0.746886           0.706195         0.708153        0.030892   \n20           0.746738           0.706456         0.708159        0.030863   \n21           0.746578           0.706717         0.708157        0.030833   \n22           0.746405           0.706975         0.708148        0.030803   \n23           0.746219           0.707232         0.708130        0.030772   \n24           0.746018           0.707484         0.708103        0.030740   \n25           0.745801           0.707732         0.708065        0.030708   \n26           0.745568           0.707974         0.708016        0.030674   \n27           0.745317           0.708208         0.707955        0.030639   \n28           0.745048           0.708435         0.707881        0.030604   \n29           0.744758           0.708651         0.707793        0.030566   \n..                ...                ...              ...             ...   \n60           0.622404           0.196931         0.427007        0.175208   \n61           0.634059           0.218160         0.440037        0.170743   \n62           0.645280           0.238909         0.452722        0.166421   \n63           0.656072           0.259172         0.465068        0.162242   \n64           0.666443           0.278946         0.477078        0.158208   \n65           0.676400           0.298230         0.488757        0.154315   \n66           0.685952           0.317024         0.500110        0.150564   \n67           0.695107           0.335328         0.511144        0.146950   \n68           0.703872           0.353147         0.521864        0.143469   \n69           0.712259           0.370484         0.532277        0.140117   \n70           0.720275           0.387344         0.542388        0.136889   \n71           0.727930           0.403733         0.552205        0.133778   \n72           0.735234           0.419660         0.561734        0.130780   \n73           0.742198           0.435131         0.570981        0.127889   \n74           0.748831           0.450155         0.579954        0.125097   \n75           0.755144           0.464741         0.588660        0.122400   \n76           0.761148           0.478899         0.597104        0.119792   \n77           0.766852           0.492638         0.605295        0.117268   \n78           0.772267           0.505970         0.613238        0.114821   \n79           0.777404           0.518904         0.620940        0.112449   \n80           0.782273           0.531450         0.628407        0.110145   \n81           0.786885           0.543618         0.635646        0.107907   \n82           0.791249           0.555420         0.642663        0.105731   \n83           0.795377           0.566865         0.649463        0.103614   \n84           0.799277           0.577963         0.656052        0.101553   \n85           0.802959           0.588724         0.662436        0.099548   \n86           0.806434           0.599156         0.668619        0.097594   \n87           0.809709           0.609271         0.674608        0.095693   \n88           0.812795           0.619075         0.680407        0.093842   \n89           0.815700           0.628578         0.686020        0.092041   \n\n    rank_test_score  split0_train_score  split1_train_score  \\\n0                60            0.765659            0.732051   \n1                59            0.765622            0.732017   \n2                58            0.765582            0.731979   \n3                57            0.765538            0.731938   \n4                56            0.765489            0.731894   \n5                55            0.765437            0.731844   \n6                54            0.765379            0.731791   \n7                53            0.765316            0.731732   \n8                52            0.765248            0.731668   \n9                50            0.765174            0.731598   \n10               48            0.765093            0.731521   \n11               47            0.765005            0.731438   \n12               45            0.764910            0.731348   \n13               44            0.764807            0.731249   \n14               42            0.764695            0.731142   \n15               40            0.764574            0.731026   \n16               39            0.764444            0.730900   \n17               37            0.764303            0.730764   \n18               35            0.764151            0.730616   \n19               33            0.763987            0.730457   \n20               31            0.763811            0.730284   \n21               32            0.763622            0.730099   \n22               34            0.763418            0.729898   \n23               36            0.763200            0.729683   \n24               38            0.762965            0.729451   \n25               41            0.762714            0.729202   \n26               43            0.762446            0.728934   \n27               46            0.762158            0.728647   \n28               49            0.761851            0.728339   \n29               51            0.761523            0.728010   \n..              ...                 ...                 ...   \n60               90            0.976990            0.983499   \n61               89            0.976510            0.983157   \n62               88            0.976021            0.982807   \n63               87            0.975524            0.982451   \n64               86            0.975019            0.982086   \n65               85            0.974504            0.981714   \n66               84            0.973981            0.981335   \n67               83            0.973449            0.980948   \n68               82            0.972908            0.980554   \n69               81            0.972358            0.980152   \n70               80            0.971800            0.979742   \n71               79            0.971232            0.979324   \n72               78            0.970655            0.978899   \n73               77            0.970069            0.978466   \n74               76            0.969474            0.978025   \n75               75            0.968870            0.977576   \n76               74            0.968257            0.977119   \n77               73            0.967635            0.976654   \n78               72            0.967004            0.976182   \n79               71            0.966364            0.975701   \n80               70            0.965716            0.975213   \n81               69            0.965059            0.974717   \n82               68            0.964393            0.974213   \n83               67            0.963718            0.973700   \n84               66            0.963036            0.973180   \n85               65            0.962345            0.972652   \n86               64            0.961647            0.972117   \n87               63            0.960941            0.971573   \n88               62            0.960227            0.971021   \n89               61            0.959506            0.970462   \n\n    split2_train_score  mean_train_score  std_train_score  \n0             0.743390          0.747033         0.013960  \n1             0.743345          0.746994         0.013960  \n2             0.743295          0.746952         0.013960  \n3             0.743240          0.746905         0.013960  \n4             0.743180          0.746854         0.013959  \n5             0.743115          0.746799         0.013959  \n6             0.743043          0.746738         0.013959  \n7             0.742966          0.746671         0.013959  \n8             0.742881          0.746599         0.013959  \n9             0.742789          0.746520         0.013959  \n10            0.742689          0.746434         0.013959  \n11            0.742580          0.746341         0.013959  \n12            0.742462          0.746240         0.013960  \n13            0.742335          0.746130         0.013960  \n14            0.742196          0.746011         0.013961  \n15            0.742047          0.745883         0.013962  \n16            0.741886          0.745743         0.013963  \n17            0.741712          0.745593         0.013965  \n18            0.741524          0.745431         0.013966  \n19            0.741322          0.745255         0.013968  \n20            0.741105          0.745067         0.013971  \n21            0.740872          0.744864         0.013974  \n22            0.740622          0.744646         0.013977  \n23            0.740354          0.744412         0.013981  \n24            0.740066          0.744161         0.013985  \n25            0.739759          0.743892         0.013990  \n26            0.739430          0.743603         0.013996  \n27            0.739080          0.743295         0.014002  \n28            0.738705          0.742965         0.014009  \n29            0.738306          0.742613         0.014017  \n..                 ...               ...              ...  \n60            0.977697          0.979396         0.002916  \n61            0.977301          0.978989         0.002965  \n62            0.976897          0.978575         0.003014  \n63            0.976485          0.978153         0.003064  \n64            0.976065          0.977723         0.003115  \n65            0.975637          0.977285         0.003166  \n66            0.975200          0.976839         0.003218  \n67            0.974754          0.976384         0.003271  \n68            0.974299          0.975920         0.003325  \n69            0.973836          0.975449         0.003380  \n70            0.973363          0.974968         0.003435  \n71            0.972880          0.974479         0.003492  \n72            0.972388          0.973981         0.003549  \n73            0.971887          0.973474         0.003607  \n74            0.971376          0.972958         0.003666  \n75            0.970854          0.972434         0.003725  \n76            0.970323          0.971900         0.003786  \n77            0.969783          0.971357         0.003847  \n78            0.969232          0.970806         0.003909  \n79            0.968671          0.970246         0.003971  \n80            0.968100          0.969676         0.004034  \n81            0.967519          0.969098         0.004098  \n82            0.966928          0.968511         0.004162  \n83            0.966327          0.967915         0.004227  \n84            0.965717          0.967311         0.004292  \n85            0.965096          0.966698         0.004358  \n86            0.964466          0.966077         0.004423  \n87            0.963827          0.965447         0.004489  \n88            0.963177          0.964809         0.004555  \n89            0.962519          0.964162         0.004621  \n\n[90 rows x 18 columns]"
                    },
                    "execution_count": 43,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "pd.DataFrame(grid.cv_results_)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Summary\n\n1. We can manually generate folds by using `KFolds`\n2. We can get a score using `cross_val_predict(X, y, cv=KFoldObject_or_integer)`. \n   This will produce the out-of-bag prediction for each row.\n3. When doing hyperparameter selection, we should be optimizing on out-of-bag scores. This means either using `cross_val_predict` in a loop, or ....\n4. .... use `GridSearchCV`. GridSearchCV takes a model (or pipeline) and a dictionary of parameters to scan over. It finds the hyperparameter set that has the best out-of-sample score on all the parameters, and calls that it's \"best estimator\". It then retrains on all data with the \"best\" hyper-parameters.\n\n### Extensions\n\nHere are some additional items to keep in mind:\n* There is a `RandomSearchCV` that tries random combination of model parameters. This can be helpful if you have a prohibitive number of combinations to test them all exhaustively.\n* KFolds will randomly select rows to be in the training and test folds. There are other methods (such as `StratifiedKFolds` and `GroupKFold`, which are useful when you need more control over how the data is split (e.g. to prevent data leakage). You can create these specialized objects and pass them to the `cv` argument of `GridSearchCV`."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "---\n### Machine Learning Foundation (C) 2020 IBM Corporation"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
